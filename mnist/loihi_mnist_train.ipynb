{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "import gzip\n",
    "import os\n",
    "import pickle\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "import nengo\n",
    "import nengo_dl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "try:\n",
    "    import requests\n",
    "    has_requests = True\n",
    "except ImportError:\n",
    "    has_requests = False\n",
    "\n",
    "import nengo_loihi\n",
    "import keras"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "def random_crop(img, crop_size=(10, 10)):\n",
    "    assert crop_size[0] <= img.shape[0] and crop_size[1] <= img.shape[1], \"Crop size should be less than image size\"\n",
    "    img = img.copy()\n",
    "    w, h = img.shape[:2]\n",
    "    x, y = np.random.randint(h-crop_size[0]), np.random.randint(w-crop_size[1])\n",
    "    img = img[y:y+crop_size[0], x:x+crop_size[1]]\n",
    "    return img\n",
    "\n",
    "def center_crop(img, crop_size=(10, 10)):\n",
    "    assert crop_size[0] <= img.shape[0] and crop_size[1] <= img.shape[1], \"Crop size should be less than image size\"\n",
    "    img = img.copy()\n",
    "    w, h = img.shape[:2]\n",
    "    x, y = int((w-crop_size[0])/2), int((h-crop_size[1])/2)\n",
    "    img = img[y:y+crop_size[0], x:x+crop_size[1]]\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = (\n",
    "    tf.keras.datasets.mnist.load_data())\n",
    "\n",
    "# flatten images\n",
    "train_images = train_images.reshape((train_images.shape[0], -1))\n",
    "test_images = test_images.reshape((test_images.shape[0], -1))\n",
    "\n",
    "train_data = [train_images/254, train_labels]\n",
    "test_data = [test_images/254, test_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for data in (train_data, test_data):\n",
    "    one_hot = np.zeros((data[0].shape[0], 10))\n",
    "    one_hot[np.arange(data[0].shape[0]), data[1]] = 1\n",
    "    data[1] = one_hot\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_data[0] = np.repeat(train_data[0], 10, axis=0)\n",
    "train_data[1] = np.repeat(train_data[1], 10, axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "temp = []\n",
    "for img in train_data[0]:\n",
    "    temp.append(random_crop(img.reshape(28,28,1),(24,24)).reshape(-1))\n",
    "train_data[0] = np.array(temp)\n",
    "temp = []\n",
    "for img in test_data[0]:\n",
    "    temp.append(center_crop(img.reshape(28,28,1),(24,24)).reshape(-1))\n",
    "test_data[0] = np.array(temp)\n",
    "temp = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def conv_layer(x, *args, activation=True, **kwargs):\n",
    "    # create a Conv2D transform with the given arguments\n",
    "    conv = nengo.Convolution(*args, channels_last=True, **kwargs)\n",
    "\n",
    "    if activation:\n",
    "        # add an ensemble to implement the activation function\n",
    "        layer = nengo.Ensemble(conv.output_shape.size, 1).neurons\n",
    "    else:\n",
    "        # no nonlinearity, so we just use a node\n",
    "        layer = nengo.Node(size_in=conv.output_shape.size)\n",
    "\n",
    "    # connect up the input object to the new layer\n",
    "    nengo.Connection(x, layer, transform=conv)\n",
    "\n",
    "    # print out the shape information for our new layer\n",
    "    print(\"LAYER\")\n",
    "    print(conv.input_shape.shape, \"->\", conv.output_shape.shape)\n",
    "    print(conv.output_shape.size)\n",
    "\n",
    "    return layer, conv"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "LAYER\n",
      "(24, 24, 1) -> (24, 24, 1)\n",
      "576\n",
      "LAYER\n",
      "(24, 24, 1) -> (7, 7, 5)\n",
      "245\n",
      "LAYER\n",
      "(24, 24, 1) -> (7, 7, 5)\n",
      "245\n",
      "LAYER\n",
      "(24, 24, 1) -> (7, 7, 5)\n",
      "245\n",
      "LAYER\n",
      "(24, 24, 1) -> (7, 7, 5)\n",
      "245\n",
      "LAYER\n",
      "(24, 24, 1) -> (7, 7, 5)\n",
      "245\n",
      "LAYER\n",
      "(24, 24, 1) -> (7, 7, 5)\n",
      "245\n",
      "LAYER\n",
      "(24, 24, 1) -> (7, 7, 5)\n",
      "245\n",
      "LAYER\n",
      "(24, 24, 1) -> (7, 7, 5)\n",
      "245\n",
      "LAYER\n",
      "(24, 24, 1) -> (7, 7, 5)\n",
      "245\n",
      "LAYER\n",
      "(24, 24, 1) -> (7, 7, 5)\n",
      "245\n",
      "LAYER\n",
      "(24, 24, 1) -> (7, 7, 5)\n",
      "245\n",
      "LAYER\n",
      "(24, 24, 1) -> (7, 7, 5)\n",
      "245\n",
      "LAYER\n",
      "(24, 24, 1) -> (7, 7, 5)\n",
      "245\n",
      "LAYER\n",
      "(24, 24, 1) -> (7, 7, 5)\n",
      "245\n",
      "LAYER\n",
      "(24, 24, 1) -> (7, 7, 5)\n",
      "245\n",
      "LAYER\n",
      "(24, 24, 1) -> (7, 7, 5)\n",
      "245\n",
      "LAYER\n",
      "10 -> 10\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "\n",
    "dt = 0.001  # simulation timestep\n",
    "presentation_time = 0.2  # input presentation time\n",
    "max_rate = 200  # neuron firing rates\n",
    "# neuron spike amplitude (scaled so that the overall output is ~1)\n",
    "amp = 1 / max_rate\n",
    "# input image shape\n",
    "input_shape = (24, 24, 1)\n",
    "n_parallel = 16\n",
    "size_out = 20\n",
    "nengo_loihi.set_defaults()\n",
    "\n",
    "with nengo.Network(seed=0) as net:\n",
    "    # set up the default parameters for ensembles/connections\n",
    "    nengo_loihi.add_params(net)\n",
    "    net.config[nengo.Ensemble].neuron_type = (\n",
    "        nengo.LIF(amplitude=amp))\n",
    "    net.config[nengo.Ensemble].max_rates = nengo.dists.Choice([max_rate])\n",
    "    net.config[nengo.Ensemble].intercepts = nengo.dists.Choice([0])\n",
    "    net.config[nengo.Connection].synapse = None\n",
    "\n",
    "    # the input node that will be used to feed in input images\n",
    "    inp = nengo.Node(\n",
    "        nengo.processes.PresentInput(test_data[0], presentation_time),\n",
    "        size_out=24 * 24 * 1)\n",
    "\n",
    "    # the output node provides the 10-dimensional classification\n",
    "    out = nengo.Node(size_in=10)\n",
    "\n",
    "    layer, conv = conv_layer(\n",
    "        inp, 1, input_shape, kernel_size=(1, 1), strides=(1, 1),\n",
    "        init=np.ones((1, 1, 1, 1)))\n",
    "\n",
    "    # first layer is off-chip to translate the images into spikes\n",
    "    net.config[layer.ensemble].on_chip = False\n",
    "\n",
    "    # build parallel copies of the network\n",
    "    for _ in range(n_parallel):\n",
    "        layer2, conv2 = conv_layer(layer, 5, conv.output_shape, kernel_size=(5, 5),\n",
    "                                   strides=(3, 3))\n",
    "        nengo.Connection(layer2, out, transform=nengo_dl.dists.Glorot())\n",
    "    print(\"LAYER\")\n",
    "    print(out.size_in, \"->\", out.size_out)\n",
    "    out_p = nengo.Probe(out)\n",
    "    out_p_filt = nengo.Probe(out, synapse=nengo.Alpha(0.016))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# set up training data\n",
    "minibatch_size = 100\n",
    "train_data_test = {\n",
    "    inp: np.tile(train_data[0][:1000, None, :],\n",
    "                 (1, int(presentation_time / dt), 1)),\n",
    "    out_p_filt: np.tile(train_data[1][:1000, None, :],\n",
    "                        (1, int(presentation_time / dt), 1))\n",
    "}\n",
    "\n",
    "train_data_dl = {inp: train_data[0][:, None, :],\n",
    "              out_p: train_data[1][:, None, :]}\n",
    "\n",
    "# for the test data evaluation we'll be running the network over time\n",
    "# using spiking neurons, so we need to repeat the input/target data\n",
    "# for a number of timesteps (based on the presentation_time)\n",
    "test_data_loihi = {\n",
    "    inp: np.tile(test_data[0][:1000, None, :],\n",
    "                 (1, int(presentation_time / dt), 1)),\n",
    "    out_p_filt: np.tile(test_data[1][:1000, None, :],\n",
    "                        (1, int(presentation_time / dt), 1))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def crossentropy(outputs, targets):\n",
    "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        logits=outputs, labels=targets))\n",
    "\n",
    "\n",
    "def classification_error(outputs, targets):\n",
    "    return 100 * tf.reduce_mean(\n",
    "        tf.cast(tf.not_equal(tf.argmax(outputs[:, -1], axis=-1),\n",
    "                             tf.argmax(targets[:, -1], axis=-1)),\n",
    "                tf.float32))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Build finished in 0:00:00                                                      \n",
      "Optimization finished in 0:00:00                                               \n",
      "Construction finished in 0:00:00                                               ##################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################| ETA: 0:00:00\n",
      "|#################Training (43%)                  | ETA: 0:01:20 (loss: 0.2729)"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "do_training = True\n",
    "\n",
    "with nengo_dl.Simulator(net, minibatch_size=minibatch_size, seed=0) as sim:\n",
    "    if do_training:\n",
    "\n",
    "        # run training\n",
    "        sim.train(train_data_dl, tf.train.RMSPropOptimizer(learning_rate=0.001),\n",
    "                  objective={out_p: crossentropy}, n_epochs=1)\n",
    "\n",
    "        sim.save_params(\"./mnist\")\n",
    "    else:\n",
    "        #download(\"mnist_params.data-00000-of-00001\",\n",
    "        #         \"1BaNU7Er_Q3SJt4i4Eqbv1Ln_TkmmCXvy\")\n",
    "        #download(\"mnist_params.index\", \"1w8GNylkamI-3yHfSe_L1-dBtvaQYjNlC\")\n",
    "        #download(\"mnist_params.meta\", \"1JiaoxIqmRupT4reQ5BrstuILQeHNffrX\")\n",
    "        sim.load_params(\"./mnist\")\n",
    "\n",
    "\n",
    "    # store trained parameters back into the network\n",
    "    sim.freeze_params(net)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for conn in net.all_connections:    \n",
    "    conn.synapse = 0.016\n",
    "\n",
    "with nengo_dl.Simulator(net, minibatch_size=minibatch_size) as sim:\n",
    "    print(\"train error w/ synapse: %.2f%%\" %\n",
    "          sim.loss(train_data_test, {out_p_filt: classification_error}))\n",
    "\n",
    "\n",
    "with nengo_dl.Simulator(net, minibatch_size=minibatch_size) as sim:\n",
    "    print(\"test error w/ synapse: %.2f%%\" %\n",
    "          sim.loss(test_data_loihi, {out_p_filt: classification_error}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "loihi error: 0.00%\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "n_presentations = 1\n",
    "#net.config[out_ens.ensemble].on_chip = False\n",
    "with nengo_loihi.Simulator(net, dt=dt, precompute=False) as sim:\n",
    "    # if running on Loihi, increase the max input spikes per step\n",
    "    if 'loihi' in sim.sims:\n",
    "        sim.sims['loihi'].snip_max_spikes_per_step = 122000\n",
    "\n",
    "    # run the simulation on Loihi\n",
    "    sim.run(n_presentations * presentation_time)\n",
    "\n",
    "    # check classification error\n",
    "    step = int(presentation_time / dt)\n",
    "    output = sim.data[out_p_filt][step - 1::step]\n",
    "    correct = 100 * (np.mean(\n",
    "        np.argmax(output, axis=-1)\n",
    "        != np.argmax(test_data_loihi[out_p_filt][:n_presentations, -1],\n",
    "                     axis=-1)\n",
    "    ))\n",
    "    print(\"loihi error: %.2f%%\" % correct)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}