{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1204 12:01:04.759284  4056 module_wrapper.py:139] From c:\\python36\\lib\\site-packages\\nengo_dl\\compat.py:26: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W1204 12:01:04.763277  4056 module_wrapper.py:139] From c:\\python36\\lib\\site-packages\\nengo_dl\\__init__.py:38: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\n",
      "W1204 12:01:04.764275  4056 module_wrapper.py:139] From c:\\python36\\lib\\site-packages\\nengo_dl\\__init__.py:38: The name tf.logging.WARN is deprecated. Please use tf.compat.v1.logging.WARN instead.\n",
      "\n",
      "W1204 12:01:04.765270  4056 module_wrapper.py:139] From c:\\python36\\lib\\site-packages\\nengo_dl\\__init__.py:43: The name tf.disable_v2_behavior is deprecated. Please use tf.compat.v1.disable_v2_behavior instead.\n",
      "\n",
      "W1204 12:01:04.767264  4056 deprecation.py:323] From c:\\python36\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import gzip\n",
    "import os\n",
    "import pickle\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "import nengo\n",
    "import nengo_dl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "try:\n",
    "    import requests\n",
    "    has_requests = True\n",
    "except ImportError:\n",
    "    has_requests = False\n",
    "\n",
    "import nengo_loihi\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAACQ1JREFUeJzt3c2LXWcdB/Dn3LnJJJk0NmnapqWhuEgXWvEFERXEVRfiogupC1cigtiV4EYX7v0DdKOgQsEKLhTBgkhBEVEoFIvUSl2Y+oZoapo075m5x4W1NGHu72lyM3devp/Pcn455565ne88Zb73OWcYx7EBe99kuy8AWA5hhxDCDiGEHUIIO4QQdggh7BBC2NnUMAzHhmH40TAMF4dheGUYhs9s9zWxmOl2XwA71jdba9daa/e31t7XWvvpMAwvjOP44vZeFrdr8Ak6bjYMw1pr7Wxr7dFxHF9+42tPtdb+Po7jV7b14rht/jeezTzSWtv4f9Df8EJr7d3bdD3cAcLOZg631s7d9LVzrbW7tuFauEOEnc1caK0duelrR1prr2/DtXCHCDubebm1Nh2G4dRbvvbe1po/zu1i/kDHpoZh+EFrbWytfb7976/xz7TWPuqv8buXlZ15nmytHWyt/au19nRr7YuCvrtZ2SGElR1CCDuEEHYIIewQYqkbYR6bPOGvgZuYPnyynP/pCw+V81Pf/sfc2fqfX7mta1qG2cffX85ffdeBcn7fd54v5+PVq7d8TXvBz2c/HDb7upUdQgg7hBB2CCHsEELYIYSwQwhhhxBuOLkEK0ePlvO/fLru2Z98/JlyfvaTa3Nnvz/3YHnsxeurnfn+cn5i7Xw5f8e+K3Nnjx39cXnsV3/1qXI+bHygnB//1m/KeRorO4QQdggh7BBC2CGEsEMIYYcQwg4h9OxLsHH2bDnff67e5v/01z9Rzj/ypefmzj77wK/LYz924Ew5P7pyqJy/eO1yOT+9Pv8zBl9+/ony2Ad/tlLOrx0ux9zEyg4hhB1CCDuEEHYIIewQQtghhOptB5jt3/TOv2+avjYr57/87ofmzvZ9bqM89j8bdX91bOVCOX/pyqly/r0/fnju7P6nDpbHnntnXb0d/Hf9vnAjKzuEEHYIIewQQtghhLBDCGGHEMIOIfTsO8C+C/UW10vH69/JR15Znzt77msfLI999uT8Hry11q4crz8DcOR03XWfODO/5790b92jz3o/nfWlcRMrO4QQdggh7BBC2CGEsEMIYYcQwg4h9Ow7wGS97tl7hfKl43VfXTl0pu7JD/+zvrbrh+r14vWH5v+IDfVW+zb03pbenBtY2SGEsEMIYYcQwg4hhB1CCDuEEHYIoWffAcZJ3aMPY10oT4q+etap4K/cvY2/73v70Ts9+mxqQ/utsLJDCGGHEMIOIYQdQgg7hBB2CKF62wGuHa4rpNlqffzKlfkd1dip3obOU497x48LtF9jZ6npzTcO3P5rJ7KyQwhhhxDCDiGEHUIIO4QQdggh7BBCz74DjJ3/Ct0uu5j3uureNtPeay9y/sn8J02/rXP3tu9yIys7hBB2CCHsEELYIYSwQwhhhxDCDiH07DtAr0+eXqrvqVztOe/uGe/06L3HKnct8FjllasLvjY3sLJDCGGHEMIOIYQdQgg7hBB2CCHsEELPvgN094R3VPu6F74v/BYuB7POT9/K1bqkv3yvRzbfCis7hBB2CCHsEELYIYSwQwhhhxDCDiH07EswPXF/Oe913b17u1d7xreyJ387qp5/Nq2/sX3Fc+dba219rZ5P1tbmv/bFi+Wxe5GVHUIIO4QQdggh7BBC2CGEsEMI1dsSjJcul/PuLZMXuB1z16LnXvSRzoXeI5n3n69fPLFeq1jZIYSwQwhhhxDCDiGEHUIIO4QQdgihZ1+Ccbz9Ry7vZUPnfdlYXdKFhLCyQwhhhxDCDiGEHUIIO4QQdggh7BBCz74Ew3Sxt7n72OUt/JW9na89Tur96sNG5wST4gMMs97Be4+VHUIIO4QQdggh7BBC2CGEsEMIYYcQevYlGNYO1f+gc+/2oTMfizq610X3evKt3Gs/Dp0evbPfvfzGW2uTgwfmzhLvKW9lhxDCDiGEHUIIO4QQdggh7BBC2CGEnn0ZOn1y9xnnnflCz1jvnXsb9Xr4nmEl9Ib8c1jZIYSwQwhhhxDCDiGEHUIIO4RQvS3DdAdXQL3absFqrqrPeltYx5X6xbvbb/fv6/yDLFZ2CCHsEELYIYSwQwhhhxDCDiGEHULo2Zehd8vkzu2eF7mV9MKPVF5k+2yru/TeI5n7J+/M7zk6f3bm1cVeexeyskMIYYcQwg4hhB1CCDuEEHYIIewQQs++BONqva+6+9jkRerorbwN9RYbNhZ7ZPPs0OodvJrdz8oOIYQdQgg7hBB2CCHsEELYIYSwQwg9+xKM+zo3OO89srl3f/Qd3JVXJuuLXfjkeu8fLHT6PcfbASGEHUIIO4QQdggh7BBC2CGE6m0Jeltc+yeox8OsOHQX/zrv3UK7V72t3zV/i+sOfoj2ltnFPwrArRB2CCHsEELYIYSwQwhhhxDCDiH07EuwsdppdXt98nrnBapHNncO3U69zwD0HmU9uV5/d6+dmt+z3/OL+tx7kZUdQgg7hBB2CCHsEELYIYSwQwhhhxB69iW4cPLAQsd3++iibq72ure29bepHifzPwQwzOqT9x5V3fv8waEznaI+jJUdQgg7hBB2CCHsEELYIYSwQwhhhxB69iWYXqn75FnntvK9+6fPqq6801X39ox3e/iOlWLPeXndrf8ZgeuH629uelrP/lZWdggh7BBC2CGEsEMIYYcQwg4hhB1C6NmX4K5nXyrnZx95tJxfvbvTJ1++5Ut6U3/PeF3y9z4DsIhLJ+qL6/XwB353eu4ssYG3skMIYYcQwg4hhB1CCDuEEHYIoXpbgo3z58v5yW+8UM5fe/w95fzy8fm/s6+vlYd2b1M92eh0cx3V+Xvba4+crru1Yz/5Qznvve9prOwQQtghhLBDCGGHEMIOIYQdQgg7hNCzL8NQd9WzixfL+ZHv/7aeF7PpAyfKY9cfvq+cXz26Ws57W1wP/nV+1z2e/lt5bO996W5Trd73cQv35u5QVnYIIewQQtghhLBDCGGHEMIOIYQdQgxjYN8IiazsEELYIYSwQwhhhxDCDiGEHUIIO4QQdggh7BBC2CGEsEMIYYcQwg4hhB1CCDuEEHYIIewQQtghhLBDCGGHEMIOIYQdQgg7hPgvV/l/SaoPFKAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = (\n",
    "    keras.datasets.fashion_mnist.load_data())\n",
    "\n",
    "# flatten images\n",
    "train_images = train_images.reshape((train_images.shape[0], -1))\n",
    "test_images = test_images.reshape((test_images.shape[0], -1))\n",
    "\n",
    "train_data = [train_images/254, train_labels]\n",
    "test_data = [test_images/254, test_labels]\n",
    "for data in (train_data, test_data):\n",
    "    one_hot = np.zeros((data[0].shape[0], 10))\n",
    "    one_hot[np.arange(data[0].shape[0]), data[1]] = 1\n",
    "    data[1] = one_hot\n",
    "\n",
    "# plot some examples\n",
    "for i in range(3):\n",
    "    plt.figure()\n",
    "    plt.imshow(np.reshape(train_data[0][i], (28, 28)))\n",
    "    plt.axis('off')\n",
    "    plt.title(str(np.argmax(train_data[1][i])));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def conv_layer(x, *args, activation=True, **kwargs):\n",
    "    # create a Conv2D transform with the given arguments\n",
    "    conv = nengo.Convolution(*args, channels_last=False, **kwargs)\n",
    "\n",
    "    if activation:\n",
    "        # add an ensemble to implement the activation function\n",
    "        layer = nengo.Ensemble(conv.output_shape.size, 1).neurons\n",
    "    else:\n",
    "        # no nonlinearity, so we just use a node\n",
    "        layer = nengo.Node(size_in=conv.output_shape.size)\n",
    "\n",
    "    # connect up the input object to the new layer\n",
    "    nengo.Connection(x, layer, transform=conv)\n",
    "\n",
    "    # print out the shape information for our new layer\n",
    "    print(\"LAYER\")\n",
    "    print(conv.input_shape.shape, \"->\", conv.output_shape.shape)\n",
    "\n",
    "    return layer, conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAYER\n",
      "(1, 28, 28) -> (1, 28, 28)\n",
      "LAYER\n",
      "(1, 28, 28) -> (16, 24, 24)\n",
      "LAYER\n",
      "(16, 24, 24) -> (24, 10, 10)\n"
     ]
    }
   ],
   "source": [
    "dt = 0.001  # simulation timestep\n",
    "presentation_time = 0.1  # input presentation time\n",
    "max_rate = 100  # neuron firing rates\n",
    "# neuron spike amplitude (scaled so that the overall output is ~1)\n",
    "amp = 1 / max_rate\n",
    "# input image shape\n",
    "input_shape = (1, 28, 28)\n",
    "\n",
    "with nengo.Network(seed=0) as net:\n",
    "    # set up the default parameters for ensembles/connections\n",
    "    nengo_loihi.add_params(net)\n",
    "    net.config[nengo.Ensemble].neuron_type = (\n",
    "        nengo.SpikingRectifiedLinear(amplitude=amp))\n",
    "    net.config[nengo.Ensemble].max_rates = nengo.dists.Choice([max_rate])\n",
    "    net.config[nengo.Ensemble].intercepts = nengo.dists.Choice([0])\n",
    "    net.config[nengo.Connection].synapse = None\n",
    "\n",
    "    # the input node that will be used to feed in input images\n",
    "    inp = nengo.Node(\n",
    "        nengo.processes.PresentInput(test_data[0], presentation_time),\n",
    "        size_out=28 * 28)\n",
    "\n",
    "    # the output node provides the 10-dimensional classification\n",
    "    out = nengo.Node(size_in=10)\n",
    "\n",
    "    # build parallel copies of the network\n",
    "    layer, conv = conv_layer(\n",
    "        inp, 1, input_shape, kernel_size=(1, 1),\n",
    "        init=np.ones((1, 1, 1, 1)))\n",
    "    # first layer is off-chip to translate the images into spikes\n",
    "    net.config[layer.ensemble].on_chip = False\n",
    "    layer, conv = conv_layer(layer, 16, conv.output_shape,\n",
    "                             strides=(1, 1), kernel_size=(5, 5))\n",
    "    layer, conv = conv_layer(layer, 24, conv.output_shape,\n",
    "                             strides=(2, 2), kernel_size=(5, 5))\n",
    "    #layer, conv = conv_layer(layer, 32, conv.output_shape,\n",
    "    #                         strides=(1, 1), kernel_size=(3, 3))\n",
    "    nengo.Connection(layer, out, transform=nengo_dl.dists.Glorot())\n",
    "\n",
    "    out_p = nengo.Probe(out)\n",
    "    out_p_filt = nengo.Probe(out, synapse=nengo.Alpha(0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# set up training data\n",
    "minibatch_size = 200\n",
    "train_data = {inp: train_data[0][:, None, :],\n",
    "              out_p: train_data[1][:, None, :]}\n",
    "\n",
    "# for the test data evaluation we'll be running the network over time\n",
    "# using spiking neurons, so we need to repeat the input/target data\n",
    "# for a number of timesteps (based on the presentation_time)\n",
    "test_data = {\n",
    "    inp: np.tile(test_data[0][:, None, :],\n",
    "                 (1, int(presentation_time / dt), 1)),\n",
    "    out_p_filt: np.tile(test_data[1][:, None, :],\n",
    "                        (1, int(presentation_time / dt), 1))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def crossentropy(outputs, targets):\n",
    "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        logits=outputs, labels=targets))\n",
    "\n",
    "\n",
    "def classification_error(outputs, targets):\n",
    "    return 100 * tf.reduce_mean(\n",
    "        tf.cast(tf.not_equal(tf.argmax(outputs[:, -1], axis=-1),\n",
    "                             tf.argmax(targets[:, -1], axis=-1)),\n",
    "                tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build finished in 0:00:00                                                      \n",
      "Optimization finished in 0:00:00                                               \n",
      "Construction finished in 0:00:00                                               ##################################################################################################################################################################################################################################################################################################################################################################################################################################################| ETA: 0:00:00\n",
      "Training finished in 0:02:12 (loss: 0.2528)                                    \n",
      "Calculation finished in 0:02:06                                                \n",
      "error after training: 88.47%\n"
     ]
    }
   ],
   "source": [
    "do_training = True\n",
    "\n",
    "with nengo_dl.Simulator(net, minibatch_size=minibatch_size, seed=0) as sim:\n",
    "    if do_training:\n",
    "        #print(\"error before training: %.2f%%\" %\n",
    "        #      sim.loss(test_data, {out_p_filt: classification_error}))\n",
    "\n",
    "        # run training\n",
    "        sim.train(train_data, tf.train.RMSPropOptimizer(learning_rate=0.001),\n",
    "                  objective={out_p: crossentropy}, n_epochs=5)\n",
    "\n",
    "        print(\"error after training: %.2f%%\" %\n",
    "              sim.loss(test_data, {out_p_filt: classification_error}))\n",
    "\n",
    "        sim.save_params(\"./mnist_params4\")\n",
    "    else:\n",
    "        #download(\"mnist_params.data-00000-of-00001\",\n",
    "        #         \"1BaNU7Er_Q3SJt4i4Eqbv1Ln_TkmmCXvy\")\n",
    "        #download(\"mnist_params.index\", \"1w8GNylkamI-3yHfSe_L1-dBtvaQYjNlC\")\n",
    "        #download(\"mnist_params.meta\", \"1JiaoxIqmRupT4reQ5BrstuILQeHNffrX\")\n",
    "        sim.load_params(\"./mnist_params4\")\n",
    "\n",
    "    # store trained parameters back into the network\n",
    "    sim.freeze_params(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build finished in 0:00:00                                                      \n",
      "Optimization finished in 0:00:00                                               \n",
      "Construction finished in 0:00:00                                               ##################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################| ETA: 0:00:00\n",
      "Calculation finished in 0:01:52                                                \n",
      "error w/ synapse: 31.08%\n"
     ]
    }
   ],
   "source": [
    "for conn in net.all_connections:\n",
    "    conn.synapse = 0.005\n",
    "\n",
    "\n",
    "with nengo_dl.Simulator(net, minibatch_size=minibatch_size) as sim:\n",
    "    print(\"error w/ synapse: %.2f%%\" %\n",
    "          sim.loss(test_data, {out_p_filt: classification_error}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "n_presentations = 50\n",
    "with nengo_loihi.Simulator(net, dt=dt, precompute=False) as sim:\n",
    "    # if running on Loihi, increase the max input spikes per step\n",
    "    if 'loihi' in sim.sims:\n",
    "        sim.sims['loihi'].snip_max_spikes_per_step = 120\n",
    "\n",
    "    # run the simulation on Loihi\n",
    "    sim.run(n_presentations * presentation_time)\n",
    "\n",
    "    # check classification error\n",
    "    step = int(presentation_time / dt)\n",
    "    output = sim.data[out_p_filt][step - 1::step]\n",
    "    correct = 100 * (np.mean(\n",
    "        np.argmax(output, axis=-1)\n",
    "        != np.argmax(test_data[out_p_filt][:n_presentations, -1],\n",
    "                     axis=-1)\n",
    "    ))\n",
    "    print(\"loihi error: %.2f%%\" % correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "n_presentations = 50\n",
    "with nengo.Simulator(net, dt=dt) as sim:\n",
    "    # if running on Loihi, increase the max input spikes per step\n",
    "    #if 'loihi' in sim.sims:\n",
    "    #    sim.sims['loihi'].snip_max_spikes_per_step = 120\n",
    "\n",
    "    # run the simulation on Loihi\n",
    "    sim.run(n_presentations * presentation_time)\n",
    "\n",
    "    # check classification error\n",
    "    step = int(presentation_time / dt)\n",
    "    output = sim.data[out_p_filt][step - 1::step]\n",
    "    correct = 100 * (np.mean(\n",
    "        np.argmax(output, axis=-1)\n",
    "        != np.argmax(test_data[out_p_filt][:n_presentations, -1],\n",
    "                     axis=-1)\n",
    "    ))\n",
    "    print(\"loihi error: %.2f%%\" % correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "n_plots = 10\n",
    "plt.figure()\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "images = test_data[inp].reshape(-1, 28, 28, 1)[::step]\n",
    "ni, nj, nc = images[0].shape\n",
    "allimage = np.zeros((ni, nj * n_plots, nc), dtype=images.dtype)\n",
    "for i, image in enumerate(images[:n_plots]):\n",
    "    allimage[:, i * nj:(i + 1) * nj] = image\n",
    "if allimage.shape[-1] == 1:\n",
    "    allimage = allimage[:, :, 0]\n",
    "plt.imshow(allimage, aspect='auto', interpolation='none', cmap='gray')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(sim.trange()[:n_plots * step], sim.data[out_p_filt][:n_plots * step])\n",
    "plt.legend(['%d' % i for i in range(10)], loc='best');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
